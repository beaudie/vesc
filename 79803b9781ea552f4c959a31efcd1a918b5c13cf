{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "a5437bd1_2db2958c",
        "filename": "src/libANGLE/renderer/vulkan/CLCommandQueueVk.cpp",
        "patchSetId": 19
      },
      "lineNbr": 411,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2024-04-12T14:30:33Z",
      "side": 1,
      "message": "This looks unusual, why finish in a thread, instead of like, call `submitCommands()` only?",
      "revId": "79803b9781ea552f4c959a31efcd1a918b5c13cf",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "88f9a437_cae852b9",
        "filename": "src/libANGLE/renderer/vulkan/CLCommandQueueVk.cpp",
        "patchSetId": 19
      },
      "lineNbr": 411,
      "author": {
        "id": 1604617
      },
      "writtenOn": "2024-04-12T19:36:55Z",
      "side": 1,
      "message": "Yeah, we had an internal discussion on this as well.\n\nHowever, there are cases where we need to perform \"final syncs\" or event status updating.\n\nOne example is `enqueueReadBuffer`, AFAIK there is no direct analogous Vulkan (`vkCmd*`) command for this. In this OpenCL API, user gives an alloced hostptr that the buffer is read into once cmd is finished. Since we batch cmds, we could have multiple of these read cmds in one batch.\n\nHow we workaround this is by doing a `vkCmdCopyBuffer` command to a \"temp staging buffer\". After we launch the cmd batch and wait for fence, we then perform a sync routine to `memcpy` these tracked staging buffer(s) to its corresponding hostptr.\n\nThe other case is when we also update cmd event\u0027s in the batch to `CL_COMPLETE` after fence wait when the user created a `cl_event` object for the associated cmd they previously enqueued.\n\nBoth above cases require us to wait for that fence before proceeding.\n`clFlush` is meant to be non-blocking version of `clFinish` API.\n\nIf we forgo this launch-thread, and do exactly what you mentioned above,\nthere\u0027s no way to perform any of the above two ops without fence wait\nafter we `submitCommands()`.",
      "parentUuid": "a5437bd1_2db2958c",
      "revId": "79803b9781ea552f4c959a31efcd1a918b5c13cf",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0c6c6d99_e301b247",
        "filename": "src/libANGLE/renderer/vulkan/CLCommandQueueVk.cpp",
        "patchSetId": 19
      },
      "lineNbr": 411,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2024-04-13T02:57:54Z",
      "side": 1,
      "message": "\u003e If we forgo this launch-thread, and do exactly what you mentioned above,\nthere\u0027s no way to perform any of the above two ops without fence wait\nafter we submitCommands()\n\nIIUC, after `clFlush`, the app doesn\u0027t have a guarantee that any of the above has actually happened _yet_, is that correct? Like there should be something the app uses to query / block / etc until the commands actually executed. I\u0027d expect _that\u0027s_ the moment you can do all the above.\n\nLike with GL, what happens is this:\n\n- App does glFenceSync, this is effectively a flush\n- Flush results in vkQueueSubmit associated with a serial, together with some garbage to clean up etc\n- If app queries sync status, we check if the serial is finished (by checking an internal fence). If it is, we do the post-submission processing (like garbage collection) before returning the status to the user\n\nYou should be able to effectively do the same thing here? That is `clFlush` results in `submitCommands()` which has associated \"extra work\" to be done when submission is finished. Whatever mechanism the app uses to actually check if submission is finished (later) would make sure the extra work is done.",
      "parentUuid": "88f9a437_cae852b9",
      "revId": "79803b9781ea552f4c959a31efcd1a918b5c13cf",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "defb958e_86d99d49",
        "filename": "src/libANGLE/renderer/vulkan/CLCommandQueueVk.cpp",
        "patchSetId": 19
      },
      "lineNbr": 411,
      "author": {
        "id": 1604617
      },
      "writtenOn": "2024-04-15T14:41:52Z",
      "side": 1,
      "message": "Ah okay, I see your point here.\n\n\u003e IIUC, after clFlush, the app doesn\u0027t have a guarantee that any of the above has actually happened yet, is that correct? Like there should be something the app uses to query / block / etc until the commands actually executed.\n\nYes, you are correct. In OpenCL, user would need to use `clGetEventInfo` and query the `CL_EVENT_COMMAND_EXECUTION_STATUS` parameter for synchronization after `clFlush`. Mainly for us, there are two areas we need to do driver-side sync/handling after fence:\n\n- Ensure we trigger all registered callbacks for `CL_RUNNING` -\u003e `CL_COMPLETE`\n- Call our `sync*()` routines for any pending resource memcpy\u0027s back to user\u0027s hostptr\n\n\u003e Whatever mechanism the app uses to actually check if submission is finished (later) would make sure the extra work is done.\n\nCorrect, although we would now be coupling above actions to an event query.\nLike performance wise, if user queued up lots of callbacks and resource reads,\ndoes it make more sense to bundle that in the clFlush submission thread vs. doing it in same event query thread once event makes it to CL_COMPLETE?\n\nI\u0027m still leaning towards former (submission-thread) approach (at least for now - given our existing CTS passrate/testing on it so far). Otherwise, I\u0027m fine to either approach here.",
      "parentUuid": "0c6c6d99_e301b247",
      "revId": "79803b9781ea552f4c959a31efcd1a918b5c13cf",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "5c1fc9c4_c673dd8a",
        "filename": "src/libANGLE/renderer/vulkan/CLCommandQueueVk.cpp",
        "patchSetId": 19
      },
      "lineNbr": 411,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2024-04-15T15:27:24Z",
      "side": 1,
      "message": "It\u0027s ok to have the work be done by a thread, we have the asyncCommandQueue feature for this exact reason. But running this as a generic worker task is not a great approach. In particular, our thread pool is not as versatile as the OS scheduler. Every task that is blocked here on a \"finish()\" call is using up a worker task and putting it to sleep. After a few flushes, we are left with no worker threads, even if no worker is actually doing anything!\n\nI understand your problem though, and we need some deeper changes before we can support it easily. So maybe this code is fine, but please add a warning that this is a hack to be cleaned up, and that it depends on https://anglebug.com/8669.\n\nA couple of notes for my understanding:\n\n* In \"Ensure we trigger all registered callbacks for CL_RUNNING -\u003e CL_COMPLETE\", do you mean the _application_ has registered these callbacks? That is, could the application spin wait until the event is CL_COMPLETE without making any calls to OpenCL at all? If that is the case, I agree the callbacks cannot be deferred and need to be automatically done. https://anglebug.com/8669 _should_ take care of that though.\n\n* About the memcpy. if there is truly no way to do this in Vulkan (including magic mmap tricker, and even if possible that way), we should make that a Vulkan extension so you can avoid the double copy (and also not need to worry about the post-submission operations being so expensive).",
      "parentUuid": "defb958e_86d99d49",
      "revId": "79803b9781ea552f4c959a31efcd1a918b5c13cf",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "380dd4c8_1e13e29b",
        "filename": "src/libANGLE/renderer/vulkan/CLCommandQueueVk.cpp",
        "patchSetId": 19
      },
      "lineNbr": 563,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2024-04-12T14:30:33Z",
      "side": 1,
      "message": "There seems to be a confusion with serials, which is likely going to lead to bugs. IIUC, this is what happens:\n\n- At some point, `mComputePassCommands` has serial N\n- Commands are recorded to `mComputePassCommands` (all resources would think they are used in serial N)\n- `flushComputePassCommands` is called, which sets `mComputePassCommands`\u0027s serial to N+1\n- `submitCommands` is called with `mComputePassCommands`\u0027s serial, which is N+1\n\nIn the above, the resources think they are used in serial N, but the renderer is tracking the submission that uses them as serial N+1. If later the resource is destroyed, it could happily get deleted if serial N is finished, even if serial N+1 is still ongoing on the GPU\n\nIf you look at ContextVk, we solve this by tracking the last flushed and last submitted serials (`mLastFlushedQueueSerial` and `mLastSubmittedQueueSerial` respectively).\n\nIn the above scenario, `flushComputePassCommands` would set `mLastFlushedQueueSerial` to N before changing `mComputePassCommands`\u0027s serial to N+1. Then `submitCommands` would making a submission with `flushComputePassCommands` (N), and record `mLastSubmittedQueueSerial` as the same (also N).\n\nThen for example the `finishInternal` function below would also wait on `mLastSubmittedQueueSerial` (N) instead of mComputePassCommands\u0027s serial (N+1).",
      "revId": "79803b9781ea552f4c959a31efcd1a918b5c13cf",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d4010d85_8197fc3b",
        "filename": "src/libANGLE/renderer/vulkan/CLCommandQueueVk.cpp",
        "patchSetId": 19
      },
      "lineNbr": 563,
      "author": {
        "id": 1604617
      },
      "writtenOn": "2024-04-12T19:36:55Z",
      "side": 1,
      "message": "\u003e If you look at ContextVk, we solve this by tracking the last flushed and last submitted serials (mLastFlushedQueueSerial and mLastSubmittedQueueSerial respectively)\n\nAh okay - I see (I think I remember seeing this in the past when we first looked into implementing this).\n\nAlthough I think we actually don\u0027t rely too much on these serials for our initial/current submission/flush flow \n(i.e. just used to satisfy `submitCommand` call as well as kind-of serve as our batch count).\n\nBut agreed on future bug(s) waiting to happen (esp. if we start updating CL execution to rely more towards serials). Will update here.",
      "parentUuid": "380dd4c8_1e13e29b",
      "revId": "79803b9781ea552f4c959a31efcd1a918b5c13cf",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    }
  ]
}