{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "7504456f_7d0af702",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 7
      },
      "lineNbr": 0,
      "author": {
        "id": 1392020
      },
      "writtenOn": "2024-08-08T22:35:50Z",
      "side": 1,
      "message": "One thing I don\u0027t quite excited is that from what I read here, a successful write actually involves write and read(verify), correct? It would be nice that write only involves write. You only check the last dword to ensure a write is successful. But I am not sure if that is robust enough though.",
      "revId": "f36c01b5f43455e7c4ffae9e568bd5cb8045ca91",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a817d4d1_cc1f3f47",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 7
      },
      "lineNbr": 0,
      "author": {
        "id": 1564492
      },
      "writtenOn": "2024-08-09T09:53:35Z",
      "side": 1,
      "message": "\u003e One thing I don\u0027t quite excited is that from what I read here, a successful write actually involves write and read(verify), correct?\n\nUnfortunately yes.\n\nActually it is: write -\u003e erase -\u003e check/restore (only if `isBlobCacheEvictsOldItemsFirst() \u003d\u003d false`).\n\n\nCheck is the most expensive (if exclude write). Erase is cheap. But overall, erase + check will be not noticeable compared to get PSO data, Compression and actual writing.\n\nCheck/restore will only take long time if cache is full and eviction happening.\n\nThe `MultifileBlobCache` here: https://android.googlesource.com/platform/frameworks/native/+/refs/heads/main/opengl/libs/EGL/MultifileBlobCache.cpp\nsupposed to evict oldest items in the `MultifileBlobCache::applyLRU()` method. However, the actual implementation simply iterates over the `unordered_map` evicting almost randomly.\n\nBecause of the possible random eviction, blob cache may evict chunks `1..N-1` when writing chunk `0` (reverse order).\n\nEven restoring missing chunks, in the second pass may trigger another eviction, that would require another check, and another check..., until all items are present, or number of missing items is not decreasing.\n\n`MultifileBlobCache` for example, evicts in the order the items appear in the `unordered_map`. Eviction order is not 100% random, but depends on the hash value. Because of that, there may be situation, when same items are evicted again and again, preventing writing all chunks, even while there is a space in the cache.\n\nIf update `MultifileBlobCache` so it has true LRU eviction, then we can set `angle_egl_blob_cache_evicts_old_items_first \u003d true` and avoid relatively expensive item checking.\n\n\u003e It would be nice that write only involves write.\n\nThis is possible if implement LRU eviction in the blob cache and set `angle_egl_blob_cache_evicts_old_items_first \u003d true`.\n\n\u003e You only check the last dword to ensure a write is successful. But I am not sure if that is robust enough though.\n\nSorry I do not quite understand what is this \"dword\"...\n\nMaybe you wanted to write \"last chunk\"?\n\nIf so, than this will be not enough. Eviction may choose to evict chunk `1` after writing chunk `0`, but keep the chunk `N-1` (writing in reverse order).\n\nUnfortunately, need to verify all chunks.\n\nI already suggested to Shahbaz to use 8MB chunks instead of 64K, this shuld decrease the overhead. Our platform supports 8MB blobs, so I will test and report the overhead with 8MB chunks.",
      "parentUuid": "7504456f_7d0af702",
      "revId": "f36c01b5f43455e7c4ffae9e568bd5cb8045ca91",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c1a8c2d5_ae5caa81",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 7
      },
      "lineNbr": 0,
      "author": {
        "id": 1564492
      },
      "writtenOn": "2024-08-09T11:56:48Z",
      "side": 1,
      "message": "Below are promissed tests with **8MB** chunks:\n\n\nPerformance overhead on Android S921B, that uses MultifileBlobCache.\n\n\n**Cache size limited to 64MB:**\n```\n    Uncompressed cache size:   64 557 308 bytes\n    Compressed cache size:     18 393 752 bytes\n    Chunk size:                 6 131 251\n    Number of chunks:                   3\n    Compression time:           2 292.620 ms (86.91 %)\n    Generate CRC time:              3.017 ms ( 0.11 %)\n    Store chunks cache:           221.544 ms ( 8.40 %)\n    Overhead:\n    Erase previous cache:         105.824 ms ( 4.01 %)\n    Verify all chunks present:     15.021 ms ( 0.57 %)\n        Sub total:                120.845 ms ( 4.58 %) \u003c- overhead\n    TOTAL:                      2 638.030 ms\n```\nTo my surprise, with 8MB chunks overhead is actually larger than with 64K chunks, when saving 64MB cache. And now, erase is the most expensive.\n\nThis is explained by the `MultifileBlobCache` implementation. It defers writing a blob to a thread. New blob is added to the \"hot cache\", which is the size of the maximum blob size. Because we use blob of the maximum size, next write forces wait for the previous write to finish. So, writing blob `2` is fast (async), but writing `1` and `0` - slow (around 100 ms per write). Erase after writing `0` chunk probably also triggers waiting for that write (need to check why). Maybe `MultifileBlobCache` can be optimized to allow more async writes at the time.\n\n**Unlimited cache size:**\n```\n    Uncompressed cache size:  264 518 908 bytes\n    Compressed cache size:     77 494 760 bytes\n    Chunk size:                 7 749 476\n    Number of chunks:                  10\n    Compression time:           9 553.730 ms (87.96 %)\n    Generate CRC time:              5.493 ms ( 0.05 %)\n    Store chunks cache:          1141.260 ms (10.51 %)\n    Overhead:\n    Erase previous cache:         125.741 ms ( 1.16 %)\n    Verify all chunks present:     34.713 ms ( 0.32 %)\n        Sub total:                160.454 ms ( 1.48 %) \u003c- overhead\n    TOTAL:                      2 638.030 ms\n```\n\nSaving larger cache data (and more chunks), relative overhead is a lot less (only 1.48 %). Absolute overhead increased not by much: `120.845 -\u003e 160.454 -\u003e +32.78%`\n\nIMO, such overhead acceptable.\n\nProblem corrupting exiting cache is not fictional. I started to solve this problem after obtained the error in `GetAndDecompressPipelineCacheVk()`, after which the entire cache data was erased.",
      "parentUuid": "a817d4d1_cc1f3f47",
      "revId": "f36c01b5f43455e7c4ffae9e568bd5cb8045ca91",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    }
  ]
}