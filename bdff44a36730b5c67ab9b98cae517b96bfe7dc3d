{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "b7a4032f_87d2504e",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1329751
      },
      "writtenOn": "2021-09-30T13:57:03Z",
      "side": 1,
      "message": "This is somewhat controversial because it makes the front-end mutex mutable in the Vulkan back-end.. let me try to figure out if there\u0027s an alternative before reviewing.",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "58e243e4_750e76cc",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1329751
      },
      "writtenOn": "2021-09-30T13:57:31Z",
      "side": 1,
      "message": "I guess I should ask - did you consider any alternative designs?",
      "parentUuid": "b7a4032f_87d2504e",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "790aeb04_8b21be9c",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2021-09-30T17:17:10Z",
      "side": 1,
      "message": "Cleanliness aside, I like the idea of the backend unlocking the mutex. We have a few places that \"wait\" on something, and if we are careful enough, we may be able to do this kind of thing more often and let threads work more in parallel.",
      "parentUuid": "58e243e4_750e76cc",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "7a0e77c0_ad0ce11f",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1297197
      },
      "writtenOn": "2021-09-30T19:17:47Z",
      "side": 1,
      "message": "\u003e I guess I should ask - did you consider any alternative designs?\n\nI didn\u0027t come up with any other designs, but you can take a look at Samsung\u0027s CL for an alternative:\nhttps://chromium-review.googlesource.com/c/angle/angle/+/3165687\n\nThis approach is better to me, since it avoids the polling.\n\nThe biggest issue to work around is that the global mutex needs to be released so another thread can call glFlush().   I\u0027m open to any other ideas you may have though.",
      "parentUuid": "790aeb04_8b21be9c",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "aebb1024_a0defcab",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1392020
      },
      "writtenOn": "2021-09-30T23:57:35Z",
      "side": 1,
      "message": "+1 with Jamie\u0027s concern. I think we should not try to unlock global mutex lock in the vulkan backend. What if someone comes in destroy context while you unlock? A safer way is to return the call back to front end with special return flag and let front end do the unlock and try to call into back end again if needed, I think.",
      "parentUuid": "7a0e77c0_ad0ce11f",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "dc1db5e3_5cc7cdfb",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2021-10-01T01:31:58Z",
      "side": 1,
      "message": "Not sure how it\u0027s possible for anyone to delete the context, given that it\u0027s current on the calling thread, but the concern is valid that once you get back from the wait, everything could have changed, so all the call stack should make sure it\u0027s not using stale data. That\u0027s probably going to be very error prone. So I take it back, I no longer like the idea! Samsung\u0027s CL seems to be doing the same though, doesn\u0027t it?\n\nSo here\u0027s an idea. We know that the caller necessarily has the command that signals the fence/event recorded, it might just be that the recorded command lives in another ContextVk, right?\n\nSo, how about in `SyncHelper::initialize`, we add a call to `contextVk-\u003eflushOutsideRenderPassCommands()`? (well that\u0027s private, so through the `on*SyncHelperInitialize` calls) Then we know that the command that signals the sync object lives in RendererVk (or rather in the PrimaryCommandBuffer waiting to be submitted). The significance here is that then we can say:\n\n        if (!mUse.getSerial().valid())\n        {\n            ANGLE_TRY(renderer-\u003esubmitFrame(...));\n        }\n        ASSERT(mUse.getSerial().valid());\n\nHow about that?",
      "parentUuid": "aebb1024_a0defcab",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "9b630d62_ec4411b6",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2021-10-01T04:38:02Z",
      "side": 1,
      "message": "Even better, `SyncHelper::initialize` can just flush which will probably reduce GPU bubbles elsewhere too. See anglebug.com/6482",
      "parentUuid": "dc1db5e3_5cc7cdfb",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b6233803_ea4385be",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1392020
      },
      "writtenOn": "2021-10-01T16:53:36Z",
      "side": 1,
      "message": "\u003eNot sure how it\u0027s possible for anyone to delete the context, given that it\u0027s current on the calling thread\nI thought the EGL sync object doe snot require caller on the context thread? But anyway, fence might get deleted etc. And we should not crash in those cases.\n\nHave SyncHelper::initialize issue flush immediately will fix the bug, but its  heavy handed. One of the main point of sync object is to able to track it\u0027s completion without issue a glFlush. As a temporary workaround that is okay, but I think longer term we do need to avoid flush at SyncHelper::initialize time, and then we will have to go back to this bug to redo a fix, which will go back to this discussion again.\n\nI just looked at Samsung\u0027s fix, they also unlock the global lock, so not good either. IMO the correct fix is return call all the way back to front end and unlock the global mutex lock and wait there without the global mutex lock.",
      "parentUuid": "9b630d62_ec4411b6",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a7337944_c4d2e48c",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 4
      },
      "lineNbr": 0,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2021-10-01T17:58:46Z",
      "side": 1,
      "message": "\u003e IMO the correct fix is return call all the way back to front end and unlock the global mutex lock and wait there without the global mutex lock.\n\nThat works, I don\u0027t disagree with that. But:\n\n\u003e Have SyncHelper::initialize issue flush immediately will fix the bug, but its  heavy handed.\n\nDidn\u0027t you add code to issue a flush at every endRP? How\u0027s this suddenly too much? Note that it could be merged with the flush of the endRP at the beginning of this function (like tell that endRP not to flush, because I\u0027ll do it just after recording one more command).\n\nUnless the RP is already closed, there is actually no extra flush, just the flush is delayed by one command.",
      "parentUuid": "b6233803_ea4385be",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b5b200d7_ba61fcf3",
        "filename": "src/libANGLE/renderer/vulkan/SyncVk.cpp",
        "patchSetId": 4
      },
      "lineNbr": 155,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2021-09-30T17:17:10Z",
      "side": 1,
      "message": "The mutex is already locked here by the front-end, and this is doubly locking it (which works as the mutex is recursive). But basically the unlock() below is just undoing the lock here, and doesn\u0027t unlock the front-end. It\u0027s the `wait_for` that does the second unlock.\n\nWhat you can do here is to add an std::defer_lock_t to the constructor, and remove the unlock() before `wait_for`.\n\n`wait_for` also requires the lock, so the `lock` after that call is unnecessary, and so is the unlock done by the constructor here.\n\nYou can probably just pass `egl::GetGlobalMutex()` straight to `wait_for` and that\u0027s it.",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "d43d1d03_ffcddb2e",
        "filename": "src/libANGLE/renderer/vulkan/SyncVk.cpp",
        "patchSetId": 4
      },
      "lineNbr": 155,
      "author": {
        "id": 1297197
      },
      "writtenOn": "2021-09-30T19:17:47Z",
      "side": 1,
      "message": "\u003e What you can do here is to add an std::defer_lock_t to the constructor, and remove the unlock() before `wait_for`.\n\nI haven\u0027t seen std::defer_lock_t before, so I\u0027ll have to read up more on that and play around with it.\n\n\u003e `wait_for` also requires the lock, so the `lock` after that call is unnecessary, and so is the unlock done by the constructor here.\n\u003e You can probably just pass `egl::GetGlobalMutex()` straight to `wait_for` and that\u0027s it.\n\nAh, interesting, that would be nice if it works and we don\u0027t need to create a new lock that unlocks when it goes out of scope.   I\u0027ll try giving that a shot.",
      "parentUuid": "b5b200d7_ba61fcf3",
      "revId": "bdff44a36730b5c67ab9b98cae517b96bfe7dc3d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    }
  ]
}