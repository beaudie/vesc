{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "c4f1cabd_de39531e",
        "filename": "/COMMIT_MSG",
        "patchSetId": 11
      },
      "lineNbr": 73,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2024-08-26T14:46:03Z",
      "side": 1,
      "message": "Ok so I thought this over, and I think my biggest concern is that this doesn\u0027t _guarantee_ recovery. It may _help_ in practice, but doesn\u0027t guarantee.\n\nIn particular, a future store to the blob cache may drop any of these chunks out of the control of this function, even if they all get in now. So the point to be robust is not when we _write_ the cache, but rather when we _read_ from it.\n\nSo first of all, this should really just be fixed in Vulkan. VK_KHR_pipeline_binary fixes it, but just in cases there are surprises, I added an item to maintenance9 so we get a proper \"split\" function eventually if we still have to continue using VkPipelineCache objects: https://gitlab.khronos.org/vulkan/vulkan/-/issues/3878\n\nThat said, maybe we can solve both of these problems _without_ any of the duplicate memory / readback+resave with your backwards writing trick. Say at some point we store the pipeline cache to the blob cache (call that version 1, or V1, and the chunks are V1_0, V1_1, ..., V1_n). Say we are storing an updated pipeline cache at version 2, or V2. Storing the chunks backwards, say the app gets killed in the middle. Now that blob cache has: `V1_0, V1_1, ..., V1_m, V2_m+1, ..., V2_n`.\n\nAnother situation is dropped chunks, say some chunks are dropped and we end up with  this: `V1_0, V1_1, ..., V1_m, V1_m+2, ..., V1_n` (that is, `V1_m+1` is dropped).\n\nIn both cases, we can observe that the prefix `V1_0, V1_1, ..., V1_m` is there, but the rest of the cache is unrecoverable. If we can reconstruct a subset of the pipeline cache with just those chunks, we can at least recover parts of the pipeline cache. This is not ideal, but it\u0027s \"better than nothing\", given that any unrelated blob cache store may destroy the entire pipeline cache (even with your fixes). The reason for writing backwards is if the entire cache doesn\u0027t fit in the blob cache, LRU would drop chunks from the end, so the prefix of V2 (if written all the way to chunk 0) is still retrievable.\n\nWhere this gets hairy is that the data recovered from chunks 0..m may have a partial blob in it, and loading that may blow up in the driver. For example, say from chunks of 0..m you recover 1000 KB of data, and the cache header and every blob in it is 64 KB (contrived example). This means that the 1000 KB of data have the header and 14 usable blobs, but also there is ~half of the next blob in there. However, this should be easy to make the driver robust against, i.e. Samsung\u0027s driver could be specifically made to make sure it drops the extra bytes that don\u0027t form a full blob.\n\nImplementing the above, there is no need for slots, erasing old contents or verifying things (which doesn\u0027t provide a guarantee anyway). The only things to change is:\n\n- Write chunks from the last to first\n- Be able to retrieve up-to-whatever-chunk-is-contiguously-present and decompress that\n- Based on a feature (knowing if the driver can handle an incomplete blob at the end), either take that data is whatever you could manage to keep and create a pipeline cache out of it, or drop it because there\u0027s no hope of recovery.\n\nThe decompress part may need some tricks, because missing the suffix of the data is probably going to confuse gzip. IIRC, it stores some data at the _end_ of the stream, which you would need to synthesize at the end of the incomplete data that\u0027s retrieved from the blob cache.",
      "range": {
        "startLine": 69,
        "startChar": 0,
        "endLine": 73,
        "endChar": 47
      },
      "revId": "fd283b619eae1810f9e41c7ffed7368a3efcc02d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f90321d7_e803324d",
        "filename": "/COMMIT_MSG",
        "patchSetId": 11
      },
      "lineNbr": 73,
      "author": {
        "id": 1564492
      },
      "writtenOn": "2024-08-27T16:46:01Z",
      "side": 1,
      "message": "\u003e my biggest concern is that this doesn\u0027t guarantee recovery. It may help in practice, but doesn\u0027t guarantee.\n\nI agree that this change does not provide 100% guarantee, however it \nnever promises this. It makes storing \"more\" robust, not 100% robust. Nothing is 100% safe in real life too, but you will always choose more safer way)\n\nIf blob cache is big enough to hold old + new data (dual slots) without eviction, then new data is guaranteed to be stored without corrupting old data.\n\nOtherwise, old data will still be corrupted in case of the problem 1 (termination). But guarantee to write all chunks (new data) if no termination (using verify or blob cache with conservative LRU).\n\n\n\u003e In particular, a future store to the blob cache may drop any of these \nchunks out of the control of this function, even if they all get in now.\n\nAgree that this problem exists, however it is a separate problem, not related to store corrupting old data.\n\nIn our use case, blob cache is exclusively used to write the VkPipelineCache data, and does not affected by this problem.\n\nI think we need to address both issues:\n- avoid store corrupting data when possible (this change).\n- verify new data is stored (only if blob cache does not have conservative LRU).\n- allow recovering if corruption happens (Vulkan driver + ANGLE change).\n\n\u003e So first of all, this should really just be fixed in Vulkan.\n\nTotally agree.\n\n\u003e Now that blob cache has: `V1_0, V1_1, ..., V1_m, V2_m+1, ..., V2_n`.\n\nThis is only true in case when new + old data size more than blob cache capacity (rare in our case - we have 256M capacity).\n\nIf new + old data do not exceed capacity we will have:\n`V1_0, V1_1, ..., V1_n, V2_m+1, ..., V2_n`.\n\nSo, we will still have `V1` thanks to dual slots.\n\nDual slots is useless only if new data is more or equal to the blob cache capacity (which should be avoided on practice).\n\nIn case when new + old data is more than capacity, but new data is less than capacity, we will preserve more of the `V1` in case of the termination, than when only using single slot.\n\n\u003e  If we can reconstruct a subset of the pipeline cache with just those \nchunks, we can at least recover parts of the pipeline cache. This is not\n ideal, but it\u0027s \"better than nothing\"\n\nAgree. This would be nice.\n\n\u003e Implementing the above, there is no need for slots, erasing old contents \nor verifying things (which doesn\u0027t provide a guarantee anyway). The only\n things to change is:\n\nProposed solution will improve nothing if driver requires full data (does not support partial load).\n\nIt does not solve Problem (1).\nWithout slots, writing new data will overwrite old data starting from last to 0 chunk. Terminating application before writing 0 chunk will partially erase old data. Because of that, we can only partially recover old data. Worst case there will be no chunks at all (because overwrite will delete old chunk first).\n\nSo IMO we still need dual slots and erasing.\n\nVerify is only used if Blob Cache does not have LRU that evicts only necessary blobs (not the current implementation, that evicts random blobs to 50% free capacity). So if we update the blob cache - then no need to add verify stage.\n\nI see your proposal as an addition to this commit - not a replacement.\nRecovering will help in cases if app terminates when old + new data is bigger than cache capacity, or if blob cache is used for other data (Shaders/Programs) and old chunks from PSO cache were evicted.\n\nIn other words, partial recover will make loading more robust, while dual slots - storing.\n\n\u003e The decompress part may need some tricks, because missing the suffix of \nthe data is probably going to confuse gzip. IIRC, it stores some data at\n the end of the stream, which you would need to synthesize at the end of the incomplete data that\u0027s retrieved from the blob cache.\n\nMay try to switch to 8M chunks and compress each individually. I do not expect big difference in performance (ratio and speed) - tests will tell...",
      "parentUuid": "c4f1cabd_de39531e",
      "range": {
        "startLine": 69,
        "startChar": 0,
        "endLine": 73,
        "endChar": 47
      },
      "revId": "fd283b619eae1810f9e41c7ffed7368a3efcc02d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4e6ba2f7_82e55f7b",
        "filename": "/COMMIT_MSG",
        "patchSetId": 11
      },
      "lineNbr": 73,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2024-08-28T18:49:12Z",
      "side": 1,
      "message": "\u003e In our use case, blob cache is exclusively used to write the VkPipelineCache data, and does not affected by this problem.\n\nHave you disabled ANGLE\u0027s program cache? Every glProgramLink also writes to the blob cache.\n\n\u003e This is only true in case when new + old data size more than blob cache capacity\n\nNot exactly, it could also happen if the application gets killed. But in particular, my example was for when you _don\u0027t_ have dual slots, so V2 would overwrite the last chunks of V1. FWIW, I have little objection to dual slot, that\u0027s obviously safer, though we should be able to support both single and dual slots (so ANGLE wouldn\u0027t thrash the cache by double storage on devices with a small blob cache).\n\n\u003e Proposed solution will improve nothing if driver requires full data\n\nRight, my proposal is to ask drivers nicely to support this pattern, which would hopefully not be a problem for Samsung.\n\n\u003e Without slots, writing new data will overwrite old data starting from last to 0 chunk. Terminating application before writing 0 chunk will partially erase old data\n\nAlso correct, again if we have a configuration that uses dual slots and enable that on Samsung (which has a big blob cache), that\u0027s totally fine!\n\n\u003e Verify is only used if Blob Cache does not have LRU that evicts only necessary blobs\n\nMy suggestion is to prioritize fixing LRU in blob cache, it\u0027s a good change for all applications!\n\n\u003e May try to switch to 8M chunks and compress each individually.\n\nOh right, that\u0027d be easier!\n\n\u003e In other words, partial recover will make loading more robust, while dual slots - storing.\n\nAgreed, let\u0027s do both?",
      "parentUuid": "f90321d7_e803324d",
      "range": {
        "startLine": 69,
        "startChar": 0,
        "endLine": 73,
        "endChar": 47
      },
      "revId": "fd283b619eae1810f9e41c7ffed7368a3efcc02d",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    }
  ]
}