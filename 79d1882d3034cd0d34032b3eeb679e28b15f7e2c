{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "2247a910_b300c5fe",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2023-03-29T18:38:47Z",
      "side": 1,
      "message": "These are impressive results, but unfortunately we can\u0027t just unlock things in the backend. This has come up numerous times in the past. See the first section of this comment where I previously explained (to another contributor) why this is dangerous: issuetracker.google.com/268091451#comment5\n\nThe issue of global and display lock is real though (with both deadlocks and performance), and I\u0027m glad you are tackling this. But let\u0027s take a step back and brainstorm a design that fixes things properly once and for all.\n\nI\u0027ll take up the action of learning how the front-end locking works, and then we can perhaps set up a meeting and talk it out. What do you think?",
      "revId": "79d1882d3034cd0d34032b3eeb679e28b15f7e2c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "80f34512_e5750baa",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1564492
      },
      "writtenOn": "2023-03-29T19:08:02Z",
      "side": 1,
      "message": "\u003e These are impressive results, but unfortunately we can\u0027t just unlock things in the backend. This has come up numerous times in the past. See the first section of this comment where I previously explained (to another contributor) why this is dangerous: issuetracker.google.com/268091451#comment5\n\nI need time to carefully read this. But I understand that it is potentially dangerous.\n\n\u003e In short, we can\u0027t just unlock/lock the mutex in the backend, because neither the backend nor the frontend are written with the assumption that another thread may access the surface during the call.\n\nWhat may happen with the Surface from other thread during ANI?\nI implemented this unlock a very long time ago and investigated before this, that it is safe to unlock the mutex.\n\nUnlocking needed not only to fix this problem with ANI, but also other similar problems. We are using these unlocks and it seems that there is no problems.\n\n\u003e The issue of global and display lock is real though (with both deadlocks and performance), and I\u0027m glad you are tackling this. But let\u0027s take a step back and brainstorm a design that fixes things properly once and for all.\n\nFor this need to know all problems first. \"proper\" solution may take too much time and I\u0027m afraid that we may not wait for it to happen.\n\n\u003e I\u0027ll take up the action of learning how the front-end locking works, and then we can perhaps set up a meeting and talk it out. What do you think?\n\nI think that we need to find what is really wrong with unlocking during ANI. And if there is something wrong - try fix this. And next time take into account that there is unlock during ANI.\nYou say \"we can\u0027t just unlock things in the backend\", but I spend time to investigate. It is not that I just decided to unlock the mutex and do not understand that it is potentially dangerous.",
      "parentUuid": "2247a910_b300c5fe",
      "revId": "79d1882d3034cd0d34032b3eeb679e28b15f7e2c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b29511f3_0f15e259",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2023-04-05T20:20:03Z",
      "side": 1,
      "message": "\u003e You say \"we can\u0027t just unlock things in the backend\", but I spend time to investigate. It is not that I just decided to unlock the mutex and do not understand that it is potentially dangerous.\n\nI don\u0027t doubt that.\n\nTake this scenario however. Say there are 2 shared contexts. We take the share context lock when accessing the contexts, so when one GL call is handled, we know it\u0027s \"atomic\". With unlock around ANI, this can happen:\n\n```\nThread 1:                                  Thread 2\n...\nBind framebuffer 0\nDraw:\n  Call syncState on many objects\n  Call syncState on framebuffer\n    Unlock Share Context Mutex\n    Calls ANI                              Call any GL function\n                                           Potentially using\n                                           shared resources, or\n                                           even making changes to\n                                           thread 1\u0027s context\n    Lock Share Context Mutex\n  Call syncState on more objects\n  Draw\n...\n```\n\nThe entirety of everything that happens in the GL call is just too complex to make sure that thread 2\u0027s GL call doesn\u0027t cause havoc in thread 1\u0027s processing of the GL call once it comes out of ANI. Even if you have meticulously validated that it\u0027s all ok now, I don\u0027t trust myself (and for that matter anybody else) not to screw it up in the future.\n\n\u003e What may happen with the Surface from other thread during ANI?\n\nAnd now I can answer this; the issue is not accesses to the _surface_ from other threads, but rather the context that was processing ANI.\n\nThat said, if the share context mutex is not unlocked it\u0027s much more reasonable, but I suspect that also doesn\u0027t actually solve your problem in any way.\n\n---\n\nWith the prepare-for-swap call we came up with in the past, we do solve the locked ANI situation (minus bugs, which I have ideas how to address) for when it happens from an EGL call (i.e. swap).\n\nI asked some Android experts by the way, blocking ANI during a GL call (clear, draw etc) ending up blocking other contexts has been a known issue for native drivers as well, and from what I gather no one figured out a safe way around it.",
      "parentUuid": "80f34512_e5750baa",
      "revId": "79d1882d3034cd0d34032b3eeb679e28b15f7e2c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "ab6797ab_bbbb188d",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1564492
      },
      "writtenOn": "2023-04-05T20:52:37Z",
      "side": 1,
      "message": "\u003eThe entirety of everything that happens in the GL call is just too complex to make sure that thread 2\u0027s GL call doesn\u0027t cause havoc in thread 1\u0027s processing of the GL call once it comes out of ANI. \n\nThen we need to move ANI to a place where unlock will not cause a havoc and will keep it that way.\n\n\u003e the issue is not accesses to the surface from other threads, but rather the context that was processing ANI.\n\n\u003e With the prepare-for-swap call we came up with in the past, we do solve the locked ANI situation (minus bugs, which I have ideas how to address) for when it happens from an EGL call (i.e. swap).\n\nprepare-for-swap access Context without lock at all. What if some other context modify that context or the shared state?\n\nHere I refactor prepare-for-swap into unlock.\nhttps://chromium-review.googlesource.com/c/angle/angle/+/4385294/17\nWhat is the difference? Better to answer in that CL.\n\nI do not understand the difference between prepare-for-swap and unlocking. Essentially it is the same thing, except prepare-for-swap makes you believe that everything is fine)",
      "parentUuid": "b29511f3_0f15e259",
      "revId": "79d1882d3034cd0d34032b3eeb679e28b15f7e2c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2e218853_a48e7289",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 11,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2023-04-05T21:42:54Z",
      "side": 1,
      "message": "\u003e prepare-for-swap access Context without lock at all. What if some other context modify that context or the shared state?\n\nWell yes, that\u0027s definitely a bug ðŸ˜Š\n\nWhat prepare-for-swap should _really_ do is purely call `vkAcquireNextImageKHR` and cache the return value. When swap actually happens, it should do everything else that follows under the lock.",
      "parentUuid": "ab6797ab_bbbb188d",
      "revId": "79d1882d3034cd0d34032b3eeb679e28b15f7e2c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "06aa5b25_eb44c83f",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 32,
      "author": {
        "id": 1300114
      },
      "writtenOn": "2023-03-29T18:38:47Z",
      "side": 1,
      "message": "This should be fixed in RenderDoc IMO. it shouldn\u0027t call `eglX` from a call to `vkY`.\n\nWe do have this mutex recursion problem with Android though, and IMO there\u0027s no way to keep things robust as long as ANGLE calls into platform code and the platform code calls back into ANGLE. I\u0027d really like to see Android fixed with this respect and we get rid of the recursiveness of the mutex altogether.",
      "revId": "79d1882d3034cd0d34032b3eeb679e28b15f7e2c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "bdf2360e_d47c7fea",
        "filename": "/COMMIT_MSG",
        "patchSetId": 4
      },
      "lineNbr": 32,
      "author": {
        "id": 1564492
      },
      "writtenOn": "2023-03-29T19:08:02Z",
      "side": 1,
      "message": "\u003e This should be fixed in RenderDoc IMO. it shouldn\u0027t call eglX from a call to vkY.\n\nThat is why I made an option to disable this.\nWhen you need RenderDoc your are not interested who is to blame. Solving this on the RenderDoc side may take time...\n\nAlso there may be other layers as RenderDoc that may have similar problem. We can\u0027t influence everyone...\n\n\u003e We do have this mutex recursion problem with Android though,\n\nNot only a recursion problem. Deadlock happened with two threads that lock two mutexes in a different order.\n\n\u003e  I\u0027d really like to see Android fixed with this respect and we get rid of the recursiveness of the mutex altogether.\n\nAndroid platform must use Vulkan for this.",
      "parentUuid": "06aa5b25_eb44c83f",
      "revId": "79d1882d3034cd0d34032b3eeb679e28b15f7e2c",
      "serverId": "3ce6091f-6c88-37e8-8c75-72f92ae8dfba"
    }
  ]
}